import argparse
import requests
import json
import numpy as np
import matplotlib.pyplot as plt
import datetime
import numpy
import timeit
import re

from time import sleep
from modify_resources import *
from weighting_conversions import *

from measure_performance_MEAN_py3 import *

from graph_plotter import *

#Amount of time to allow commands to propagate through system
COMMAND_DELAY = 3

### Start the stresses on the various resources

def start_causal_cpu(ssh_client, increment):
    throttle_network(ssh_client, increment)
    throttle_disk(ssh_client, increment)
    sleep(COMMAND_DELAY)
    
def start_causal_disk(ssh_client, increment):
    throttle_cpu(ssh_client, increment)
    throttle_network(ssh_client, increment)
    sleep(COMMAND_DELAY)

def start_causal_network(ssh_client, increment):
    throttle_cpu(ssh_client, increment)
    throttle_disk(ssh_client, increment)
    sleep(COMMAND_DELAY)

### Throttle only a single resource at a time.
def throttle_cpu(ssh_client, increment):
    cpu_throttle_rate = weighting_to_cpu_period(increment)
    print 'CPU Reduction Rate: {}'.format(cpu_throttle_rate)
    set_cpu_shares(ssh_client, cpu_throttle_rate)

def throttle_disk(ssh_client, increment):
    disk_throttle_rate = weighting_to_disk_access_rate(increment)
    print 'Disk Throttle Rate: {}'.format(disk_throttle_rate)
    create_dummy_disk_eater(ssh_client, disk_throttle_rate)

def throttle_network(ssh_client, increment):
    container_to_network_capacity = get_container_network_capacity(ssh_client)
    network_reduction_rate = weighting_to_bandwidth(ssh_client, increment, container_to_network_capacity)
    print 'Network Reduction Rate: {}'.format(network_reduction_rate)
    set_network_bandwidth(ssh_client, network_reduction_rate)

###Stop the throttling for a single resource
def stop_throttle_cpu(ssh_client):
    reset_cpu(ssh_client, 'period')

def stop_throttle_network(ssh_client):
    remove_all_network_manipulation(ssh_client)

def stop_throttle_disk(ssh_client):
    remove_dummy_disk_eater(ssh_client)    

### Revert system to the initial state

def stop_causal_cpu(ssh_client):
    stop_throttle_network(ssh_client)
    stop_throttle_disk(ssh_client)
    sleep(COMMAND_DELAY)
    
def stop_causal_disk(ssh_client):
    stop_throttle_network(ssh_client)
    stop_throttle_cpu(ssh_client)
    sleep(COMMAND_DELAY)

def stop_causal_network(ssh_client):
    stop_throttle_cpu(ssh_client)
    stop_throttle_disk(ssh_client)
    sleep(COMMAND_DELAY)

def reset_all_stresses(ssh_client):
    print ('RESETTING ALL STRESSES!')
    stop_throttle_cpu(ssh_client)
    stop_throttle_disk(ssh_client)
    stop_throttle_network(ssh_client)
    sleep(COMMAND_DELAY)

#Measure response time for MEAN Application
def measure_REST_response_time(REST_args, iterations):
    REST_server_ip = REST_args[0]
    ssh_server_ip = REST_args[1]
    #Each iteration actually represents 100 web requests
    all_requests = []
    for x in range(iterations):
        all_requests += POST_to_website(REST_server_ip, 100, num_threads=4, remote=True, ssh_ip=ssh_server_ip)
        clear_all_entries(REST_server_ip)
    numpy_all_requests = numpy.array(all_requests)
    mean = numpy.mean(numpy_all_requests)
    std = numpy.std(numpy_all_requests)
    #percentile99 = numpy.percentile(a, 99)
    return numpy.array(all_requests)

#Removes outlier points from the plot
def is_outlier(points, threshold=3.5):
    points = np.array(points)
    if len(points.shape) == 1:
        points = points[:,None]
    median = np.median(points, axis=0)
    diff = np.sum((points - median)**2, axis=-1)
    diff = np.sqrt(diff)
    med_abs_deviation = np.median(diff)
    
    modified_z_score = 0.6745 * diff / med_abs_deviation
    
    return modified_z_score > threshold

#Measure response time for Spark ML-Matrix
def measure_ml_matrix(spark_args, experiment_iterations):
        spark_master_public_ip = spark_args[0]
        spark_master_private_ip = spark_args[1]
        ssh_client = quilt_ssh(spark_master_public_ip)
        spark_class = '--class edu.berkeley.cs.amplab.mlmatrix.BlockCoordinateDescent '
        driver_class = '--driver-class-path ml-matrix/target/scala-2.10/mlmatrix-assembly-0.1.jar ml-matrix-master/target/scala-2.10/mlmatrix-assembly-0.1.1.jar '
        driver_memory = '--driver-memory 4G '
        executor_memory = '--executor-memory 4G '
        spark_master = 'spark://{}:7077 '.format(spark_master_private_ip)
        ml_matrix_args = '800 6 1024 3 1'

        master_container_name_cmd = 'docker ps | grep master | awk {{\'print $11\'}}'
        _, container_name, _ = ssh_client.exec_command(master_container_name_cmd)

        #FIX ME
        container_name = 'mad_bohr'
        spark_submit_cmd = 'spark/bin/spark-submit ' + driver_memory + executor_memory + spark_class + driver_class + spark_master + ml_matrix_args

        execute_spark_job = 'docker exec {} {}'.format(container_name, spark_submit_cmd)
#        execute_spark_job = 'docker exec {} {}'.format(container_name.read().strip('\n'), spark_submit_cmd)
        print execute_spark_job

        all_runtimes = []
        utilization_diffs = []
        #Run the experiment experiment_iteration number of times
        for x in range(experiment_iterations):
            initial_utilizations = get_all_throttled_utilizations(ssh_client)
            print 'INITIAL UTILIZATIONS: {}'.format(initial_utilizations)
            _, runtime, _ = ssh_client.exec_command(execute_spark_job)
            print 'about to print the runtime read'
            result_time = runtime.read()
            print result_time
            try:
                runtime = int(re.findall(r'\d+',  result_time)[0])
            except IndexError:
                print 'Spark out of memory!'
                return (0,0)
            all_runtimes.append(runtime)
            #Returns in milliseconds
            print 'iteration {} complete'.format(x)
            final_utilizations = get_all_throttled_utilizations(ssh_client)
            print 'FINAL UTILIZATIONS: {}'.format(final_utilizations)
            utilization_diffs.append(get_utilization_diff(initial_utilizations, final_utilizations))
            
        numpy_all_requests = numpy.array(all_runtimes)

        mean = numpy.mean(numpy_all_requests)
        std = numpy.std(numpy_all_requests)
        return numpy_all_requests, utilization_diffs

#Resets all parameters of the experiment to default values
def reset_experiment(vm_ip):
    ssh_client = quilt_ssh(vm_ip)
    reset_all_stresses(ssh_client)
    try:
        clear_all_entries(vm_ip)
    except:
        print ("Couldn't reset VM {}".format(vm_ip))

#Measure the performance of the application in term of latency
def measure_runtime(experiment_args, experiment_iterations, experiment_type):
    if experiment_type == 'spark-ml-matrix':
        return measure_ml_matrix(experiment_args, experiment_iterations)
    if experiment_type == 'REST':
        return measure_REST_response_time(experiment_args, experiment_iterations)
    else:
        print ('INVALID EXPERIMENT TYPE')
        exit()

def model_machine(victim_machine, experiment_args, experiment_iterations, experiment_type, use_causal_analysis=True):
    ssh_client = quilt_ssh(victim_machine)
    #Start by clearing all the previous perturbations in case something went wrong
    
    initialize_machine(ssh_client)
    reset_all_stresses(ssh_client)
    
    increments = [60, 80]#[20, 40, 60, 80]
    reduction_level_to_latency_network = {}
    reduction_level_to_latency_disk = {}
    reduction_level_to_latency_cpu = {}

    reduction_level_to_utilization_network = {}
    reduction_level_to_utilization_disk = {}
    reduction_level_to_utilization_cpu = {}
    
    #Take baseline measurements: no perturbations!'''
    baseline_runtime_array, baseline_utilization_diff = measure_runtime(experiment_args, experiment_iterations, experiment_type)
    reduction_level_to_latency_network[0] = baseline_runtime_array
    reduction_level_to_latency_disk[0] = baseline_runtime_array
    reduction_level_to_latency_cpu[0] = baseline_runtime_array

    reduction_level_to_utilization_network[0] = baseline_utilization_diff
    reduction_level_to_utilization_disk[0] = baseline_utilization_diff
    reduction_level_to_utilization_cpu[0] = baseline_utilization_diff
    
    for increment in increments:
        print 'Experiment with increment={}'.format(increment)

        print '====================================='
        print 'INITIATING CPU Experiment'
        if use_causal_analysis:
            start_causal_cpu(ssh_client, increment)
        else:
            throttle_cpu(ssh_client, increment)
        results_data_cpu, cpu_utilization_diff = measure_runtime(experiment_args, experiment_iterations, experiment_type)
        if use_causal_analysis:
            stop_causal_cpu(ssh_client)
        else:
            stop_throttle_cpu(ssh_client)
        reduction_level_to_latency_cpu[increment] = results_data_cpu
        reduction_level_to_utilization_cpu[increment] = cpu_utilization_diff

        print '======================================'
        print 'INITIATING Network Experiment'
        if use_causal_analysis:
            start_causal_network(ssh_client, increment)
        else:
            throttle_network(ssh_client, increment)
        results_data_network, network_utilization_diff = measure_runtime(experiment_args, experiment_iterations, experiment_type)
        if use_causal_analysis:
            stop_causal_network(ssh_client)
        else:
            stop_throttle_network(ssh_client)
        reduction_level_to_latency_network[increment] = results_data_network
        reduction_level_to_utilization_network[increment] = network_utilization_diff

        print '======================================='
	print 'INITIATING Disk Experiment '
        if use_causal_analysis:
 	    start_causal_disk(ssh_client, increment)
        else:
            throttle_disk(ssh_client, increment)
 	results_data_disk, disk_utilization_diff = measure_runtime(experiment_args, experiment_iterations, experiment_type)
        if use_causal_analysis:
 	    stop_causal_disk(ssh_client)
        else:
            stop_throttle_disk(ssh_client)
 	reduction_level_to_latency_disk[increment] = results_data_disk
        reduction_level_to_utilization_disk[increment] = disk_utilization_diff

    append_results_to_file(reduction_level_to_latency_cpu, reduction_level_to_latency_disk, reduction_level_to_latency_network)

    if use_causal_analysis:
        for key in sorted(results.iterkeys()):
            if key != 0:
                reduction_level_to_latency_disk = calculate_total_delay_added(reduction_level_to_latency_disk, reduction_level_to_utilization_disk, key, 'Disk')
                reduction_level_to_latency_cpu = calculate_total_delay_added(reduction_level_to_latency_cpu, reduction_level_to_utilization_cpu, key, 'CPU')
                reduction_level_to_latency_network = calculate_total_delay_added(reduction_level_to_latency_network, reduction_level_to_utilization_network, key, 'Network')
            
    return reduction_level_to_latency_cpu, reduction_level_to_latency_disk, reduction_level_to_latency_network

def calculate_total_delay_added(results, results_diff, increment, resource_field):
    print '======================='
    print 'RESOURCE FIELD IS {}'.format(resource_field)
    print 'INCREMENT IS {}'. format(increment)
    print 'results: {}\n '.format(results)
    print 'results_diff: {}\n'.format(results_diff)
    for iteration_count in range(len(results[increment])):
        total_delay_added = 0 
        if resource_field == 'CPU':
            total_delay_added += get_disk_throttle_time(ssh_client, results_diff[increment][iteration_count]['disk'], increment)
            print 'DISK ADDED IS {}'.format(total_delay_added)
            network_time = get_network_throttle_time(ssh_client, results_diff[increment][iteration_count]['network_outbound'], increment)
            network_time += get_network_throttle_time(ssh_client, results_diff[increment][iteration_count]['network_inbound'], increment)
            print 'Network time added is {}'.format(network_time)
            total_delay_added += network_time
        elif resource_field == 'Disk':
            total_delay_added += get_cpu_throttle_time(ssh_client, results_diff[increment][iteration_count]['cpu'], increment)
            print 'CPU ADDED IS {}'.format(total_delay_added)
            network_time = get_network_throttle_time(ssh_client, results_diff[increment][iteration_count]['network_outbound'], increment)
            network_time  += get_network_throttle_time(ssh_client, results_diff[increment][iteration_count]['network_inbound'], increment)
            print 'Network time added is {}'.format(network_time)
            total_delay_added += network_time
        elif resource_field == 'Network':
            total_delay_added += get_cpu_throttle_time(ssh_client, results_diff[increment][iteration_count]['cpu'], increment)
            print 'CPU ADDED IS {}'.format(total_delay_added)
            disk_delay= get_disk_throttle_time(ssh_client, results_diff[increment][iteration_count]['disk'], increment)
            print 'Disk added is {}'.format(disk_delay)
            total_delay_added += disk_delay
            
        print 'TOTAL DELAY ADDED {} seconds'.format(total_delay_added)
        #Convert total_delay_added from seconds to milliseconds
        total_delay_added *= 1000
        results[increment][iteration_count] = results[increment][iteration_count] - total_delay_added

    print 'new results is {}'.format(results)
    print '================================='

    return results

def plot_results(ssh_client, results, resource_field, convertToMilli=True, use_causal_analysis=True):
    fig = plt.figure(1, figsize=(9,6))
    ax = fig.add_subplot(111)
    axis_labels = []
    box_array = []

    #Collect baseline "seconds" contributed by each resource
    for key in sorted(results.iterkeys()):
        axis_labels.append(key)
        if convertToMilli is True:
            results[key] = [x * 1000 for x in results[key]]
        try:
            if len(results[key]) != 0:
                temp_array = np.array(results[key])
                no_outliers = temp_array[~is_outlier(results[key])]
                box_array.append(no_outliers)
        except:
            continue
        second_temp = np.array(results[key])
                
    with open("stats", "w") as f:
        f.write("MEAN,STD\n")
        f.write("{},{}".format(np.mean(second_temp),np.std(second_temp)))

    axis_labels.sort()
    axis_labels = [str(x) for x in axis_labels]
    ax.set_xticklabels(axis_labels)
    ax.get_xaxis().tick_bottom()
    ax.get_yaxis().tick_left()
    plt.ylabel('Latency (ms)')
    plt.xlabel('Stress Setting')
    if use_causal_analysis:
        plt.title('Stressing all resources except {}'.format(resource_field))
    else:
        plt.title('Stressing only {}'.format(resource_field))
    ax.boxplot(box_array)

    #Also output the mean and the standard deviation
    for key in sorted(results.iterkeys()):
        print 'For Key: {}\n'.format(key)
        print 'Mean: {}\n'.format(numpy.mean(results[key]))
        print 'Standard Deviation: {}\n'.format(numpy.std(results[key]))

    plt.show()
    plt.savefig("except_{}.png".format(resource_field))
    
    return


#Long Running Experiment so continually write the results to disk
def append_results_to_file(results_cpu, results_disk, results_network):
    output_filename = 'results_spark.txt'
    f = open(output_filename, 'a')
    f.write('\nDisk Slowdown\n')
    f.write(str(results_disk))
    f.write('\nNetwork Slowdown\n')
    f.write(str(results_network))
    f.write('\nCPU slowdown\n')
    f.write(str(results_cpu))
    f.close()

'''
Experiment arguements takes a list of arguments for the type of experiments
Examples:
"REST": Node TODO App: [public_vm_ip]
"spark-ml-matrix": Spark ml-matrix: [public_vm_ip, private_vm_ip]
'''
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("website_ip")
    parser.add_argument("single_vm_public_ip")
    parser.add_argument("ssh_vm_public_ip")
    parser.add_argument("single_vm_private_ip")
    parser.add_argument("experiment_type")
    parser.add_argument("--iterations", type=int, default=10, help="Number of HTTP requests to send the REST server per experiment")
    parser.add_argument("--single_machine_ip", action="store_true", help="Experiment mode: IP Address of machine for single machine experiments")
    parser.add_argument("--use_causal_analysis", action="store_true", help="Set this option to stress only a single variable")
    
    args = parser.parse_args()

    master_private_ip = '10.197.59.142'
    master_public_ip = '54.153.91.29'

    ssh_client = quilt_ssh(args.single_vm_public_ip)
    get_container_network_capacity(ssh_client)

    results_disk = {}
    results_cpu = {}
    results_network = {}

    #MESSY TODO: Write an abstract class for the experiment type and implement elsewhere
    if args.experiment_type == 'REST':
        experiment_args = [args.website_ip, args.ssh_vm_public_ip]
        results_disk, results_cpu, results_network = model_machine(args.single_vm_public_ip, experiment_args, args.iterations, args.experiment_type, args.use_causal_analysis)
    elif(args.experiment_type == "spark-ml-matrix"):
        experiment_args = [args.single_vm_public_ip, args.single_vm_private_ip]
        results_cpu, results_disk, results_network = model_machine(args.single_vm_public_ip, experiment_args, args.iterations, args.experiment_type, args.use_causal_analysis)
        
    if args.experiment_type == 'REST':
        reset_experiment(args.single_vm_public_ip)

    results_in_milli = True
    if args.experiment_type == 'spark-ml-matrix':
        results_in_milli = False

    ssh_client = quilt_ssh(args.single_vm_public_ip)
    
    plot_results(ssh_client, results_disk, 'Disk', convertToMilli=results_in_milli, use_causal_analysis=args.use_causal_analysis)
    plot_results(ssh_client, results_cpu, 'CPU', convertToMilli=results_in_milli, use_causal_analysis=args.use_causal_analysis)
    plot_results(ssh_client, results_network, 'Network', convertToMilli=results_in_milli, use_causal_analysis=args.use_causal_analysis)

    append_results_to_file(results_cpu, results_disk, results_network)
    
    



    
